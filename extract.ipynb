{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Datasets and Establish Benchmark\n",
    "\n",
    "**Learning Objectives**\n",
    "- Divide into Train, Evaluation and Test datasets\n",
    "- Understand why we need each\n",
    "- Pull data out of BigQuery and into CSV\n",
    "- Establish Rules Based Benchmark\n",
    "\n",
    "## Introduction \n",
    "In the previous notebook we demonstrated how to do ML in BigQuery. However BQML is limited to linear models.\n",
    "\n",
    "For advanced ML we need to pull the data out of BigQuery and load it into a ML Framework, in our case TensorFlow.\n",
    "\n",
    "While TensorFlow [can read from BigQuery directly](https://www.tensorflow.org/api_docs/python/tf/contrib/cloud/BigQueryReader), the performance is slow. The best practice is to first stage the BigQuery files as .csv files, and then read the .csv files into TensorFlow. \n",
    "\n",
    "The .csv files can reside on local disk if we're training locally, but if we're training in the cloud we'll need to move the .csv files to the cloud, in our case Google Cloud Storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up environment variables and load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"vijays-sandbox\"  # Replace with your PROJECT\n",
    "REGION = \"us-central1\"            # Choose an available region for Cloud MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = 'service_account_key.json' # for local ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "In the [a_sample_explore_clean](a_sample_explore_clean.ipynb) notebook we came up with the following query to extract a repeatable and clean sample: \n",
    "<pre>\n",
    "#standardSQL\n",
    "SELECT\n",
    "  (tolls_amount + fare_amount) AS fare_amount, -- label\n",
    "  pickup_datetime,\n",
    "  pickup_longitude, \n",
    "  pickup_latitude, \n",
    "  dropoff_longitude, \n",
    "  dropoff_latitude\n",
    "FROM\n",
    "  `nyc-tlc.yellow.trips`\n",
    "WHERE\n",
    "  -- Clean Data\n",
    "  trip_distance > 0\n",
    "  AND passenger_count > 0\n",
    "  AND fare_amount >= 2.5\n",
    "  AND pickup_longitude > -78\n",
    "  AND pickup_longitude < -70\n",
    "  AND dropoff_longitude > -78\n",
    "  AND dropoff_longitude < -70\n",
    "  AND pickup_latitude > 37\n",
    "  AND pickup_latitude < 45\n",
    "  AND dropoff_latitude > 37\n",
    "  AND dropoff_latitude < 45\n",
    "  -- repeatable 1/5000th sample\n",
    "  AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))),5000) = 1\n",
    "  </pre>\n",
    "  \n",
    "We will use the same query **with one change**. Instead of using `pickup_datetime` as is, we will extract `dayofweek` and `hourofday` from it. This is to give us some categorical features in our dataset so we can illustrate how to deal with them when we get to feature engineering. The new query will be:\n",
    "\n",
    "<pre>\n",
    "SELECT\n",
    "  (tolls_amount + fare_amount) AS fare_amount, -- label\n",
    "  EXTRACT(DAYOFWEEK from pickup_datetime) AS dayofweek,\n",
    "  EXTRACT(HOUR from pickup_datetime) AS hourofday,\n",
    "  pickup_longitude, \n",
    "  pickup_latitude, \n",
    "  dropoff_longitude, \n",
    "  dropoff_latitude\n",
    "-- rest same as before\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train, evaluation, and test sets\n",
    "\n",
    "For ML modeling we need not just one, but three datasets.\n",
    "\n",
    "**Train:** This is what our model learns on\n",
    "\n",
    "**Evaluation (aka Validation):** We shouldn't evaluate our model on the same data we trained on because then we couldn't know whether it was memorizing the input data or whether it was generalizing. Therefore we evaluate on the evaluation dataset, aka validation dataset.\n",
    "\n",
    "**Test:** We use our evaluation dataset to tune our hyperparameters (we'll cover hyperparameter tuning in a future lesson). We need to know that our chosen set of hyperparameters will work well for data we haven't seen before because in production, that will be the case. For this reason, we create a third dataset that we never use during the model development process. We only evaluate on this once our model development is finished. Data scientists don't always create a test dataset (aka holdout dataset), but to be thorough you should.\n",
    "\n",
    "We can divide our existing 1/5000th sample three ways 70%/15%/15%  (or whatever split we like) with some modulo math demonstrated below.\n",
    "\n",
    "Because we are using a hash function these results are deterministic, we'll get the same exact split every time the query is run (assuming the underlying data hasn't changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query(phase, sample_size):\n",
    "    basequery = \"\"\"\n",
    "    SELECT\n",
    "        fare_amount,\n",
    "        EXTRACT(DAYOFWEEK from pickup_datetime) AS dayofweek,\n",
    "        EXTRACT(HOUR from pickup_datetime) AS hourofday,\n",
    "        pickuplon,\n",
    "        pickuplat,\n",
    "        dropofflon,\n",
    "        dropofflat,\n",
    "        trips_last_5min\n",
    "    FROM\n",
    "        `vijays-sandbox.taxifare.traffic`\n",
    "    WHERE\n",
    "        trip_distance > 0\n",
    "        AND fare_amount >= 2.5\n",
    "        AND pickuplon > -78\n",
    "        AND pickuplon < -70\n",
    "        AND dropofflon > -78\n",
    "        AND dropofflon < -70\n",
    "        AND pickuplat > 37\n",
    "        AND pickuplat < 45\n",
    "        AND dropofflat > 37\n",
    "        AND dropofflat < 45\n",
    "        AND passenger_count > 0\n",
    "        AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), EVERY_N) = 1\n",
    "    \"\"\"\n",
    "\n",
    "    if phase == \"TRAIN\":\n",
    "        subsample = \"\"\"\n",
    "        AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), EVERY_N * 100) >= (EVERY_N * 0)\n",
    "        AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), EVERY_N * 100) <  (EVERY_N * 70)\n",
    "        \"\"\"\n",
    "    elif phase == \"VALID\":\n",
    "        subsample = \"\"\"\n",
    "        AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), EVERY_N * 100) >= (EVERY_N * 70)\n",
    "        AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), EVERY_N * 100) <  (EVERY_N * 85)\n",
    "        \"\"\"\n",
    "    elif phase == \"TEST\":\n",
    "        subsample = \"\"\"\n",
    "        AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), EVERY_N * 100) >= (EVERY_N * 85)\n",
    "        AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), EVERY_N * 100) <  (EVERY_N * 100)\n",
    "        \"\"\"\n",
    "\n",
    "    query = basequery + subsample\n",
    "    return query.replace(\"EVERY_N\", sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to CSV\n",
    "Now let's execute a query for train/valid/test and write the results to disk in csv format. We use Pandas's `.to_csv()` method to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 151615 lines to taxi-train.csv\n",
      "Wrote 31820 lines to taxi-valid.csv\n",
      "Wrote 34080 lines to taxi-test.csv\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "bq = bigquery.Client(project=PROJECT)\n",
    "\n",
    "for phase in [\"TRAIN\", \"VALID\", \"TEST\"]:\n",
    "    # 1. Create query string\n",
    "    query_string = create_query(phase, \"5000\")\n",
    "    # 2. Load results into DataFrame\n",
    "    df = bq.query(query_string).to_dataframe()\n",
    "\n",
    "    # 3. Write DataFrame to CSV\n",
    "    df.to_csv(\"taxi-{}.csv\".format(phase.lower()), index_label = False, index = False)\n",
    "    print(\"Wrote {} lines to {}\".format(len(df), \"taxi-{}.csv\".format(phase.lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even with a 1/5000th sample we have a good amount of data for ML. 150K training examples and 30K validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that datasets exist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 reddyv  google  1967394 Jul 29 13:51 taxi-test.csv\n",
      "-rw-r--r--  1 reddyv  google  8734343 Jul 29 13:49 taxi-train.csv\n",
      "-rw-r--r--  1 reddyv  google  1830429 Jul 29 13:50 taxi-valid.csv\n"
     ]
    }
   ],
   "source": [
    "!ls -l *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview one of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fare_amount,dayofweek,hourofday,pickuplon,pickuplat,dropofflon,dropofflat,trips_last_5min\n",
      "6.1,2,0,-73.986895,40.729723,-74.00631,40.739407,1129\n",
      "9.7,7,0,-73.945783,40.777807,-73.97539,40.757712,2876\n",
      "5.3,6,0,-74.00644,40.739349,-73.999379,40.731804,3950\n",
      "7.3,5,0,-73.966118,40.753983,-73.945605,40.782802,1334\n",
      "6.5,7,0,-73.974153,40.762767,-73.989152,40.742727,2623\n",
      "22.9,1,0,-73.977188,40.774063,-73.962647,40.654768,2833\n",
      "22.9,2,0,-74.00188,40.745947,-73.968497,40.639375,2002\n",
      "6.1,3,0,-73.994051,40.751077,-73.977333,40.778875,661\n",
      "5.3,5,0,-73.980898,40.744515,-73.973383,40.753497,1938\n"
     ]
    }
   ],
   "source": [
    "!head taxi-train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! We now have our ML datasets and are ready to train ML models, validate them and test them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
